{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cfde2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.fft import rfft, irfft\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, '..')))\n",
    "\n",
    "from HOSim.solver import f\n",
    "f_jit = jax.jit(f, static_argnums=(2, 3, 4, 5, 6))\n",
    "\n",
    "h5_files = glob.glob(os.path.join(\"..\\\\output\", \"*.h5\"))\n",
    "\n",
    "eta_hat, phi_hat, Hs, Tp, modes, time, length, x = None, None, None, None, None, None, None, None\n",
    "\n",
    "for i, file in enumerate(h5_files):\n",
    "    with h5py.File(file, \"r\") as data:\n",
    "        eta_hat = data[\"eta_hat\"][:]\n",
    "        phi_hat = data[\"phi_hat\"][:]\n",
    "        Hs = data[\"Hs\"][:]\n",
    "        Tp = data[\"Tp\"][:]\n",
    "        time = data[\"time\"][:]\n",
    "\n",
    "        modes = data.attrs[\"modes\"]\n",
    "        length = data.attrs[\"length\"]\n",
    "        Ta = data.attrs[\"Ta\"]\n",
    "        x = np.linspace(0, length, 2*modes)\n",
    "\n",
    "    break\n",
    "\n",
    "index = np.argmin(np.abs(time - 2*Ta))\n",
    "\n",
    "eta_hat = eta_hat[:, index:, :]\n",
    "phi_hat = phi_hat[:, index:, :]\n",
    "time = time[index:] - time[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c70e743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 loss = 69939.578125\n",
      "Epoch    1 loss = 88591.718750\n",
      "Epoch    2 loss = 91115.648438\n",
      "Epoch    3 loss = 90060.375000\n",
      "Epoch    4 loss = 91569.617188\n",
      "Epoch    5 loss = 90367.218750\n",
      "Epoch    6 loss = 89532.710938\n",
      "Epoch    7 loss = 90590.343750\n",
      "Epoch    8 loss = 89536.140625\n",
      "Epoch    9 loss = 89774.835938\n",
      "Epoch   10 loss = 89206.367188\n",
      "Epoch   11 loss = 85601.859375\n",
      "Epoch   12 loss = 83039.476562\n",
      "Epoch   13 loss = 81603.359375\n",
      "Epoch   14 loss = 77887.789062\n",
      "Epoch   15 loss = 77579.640625\n",
      "Epoch   16 loss = 42331.128906\n",
      "Epoch   17 loss = 18156.230469\n",
      "Epoch   18 loss = 35696.414062\n",
      "Epoch   19 loss = 29407.091797\n",
      "Epoch   20 loss = 27663.591797\n",
      "Epoch   21 loss = 18646.773438\n",
      "Epoch   22 loss = 15715.579102\n",
      "Epoch   23 loss = 10628.051758\n",
      "Epoch   24 loss = 9951.967773\n",
      "Epoch   25 loss = 9289.284180\n",
      "Epoch   26 loss = 8237.560547\n",
      "Epoch   27 loss = 6592.616211\n",
      "Epoch   28 loss = 6149.274902\n",
      "Epoch   29 loss = 4980.509766\n",
      "Epoch   30 loss = 5312.329102\n",
      "Epoch   31 loss = 5109.109375\n",
      "Epoch   32 loss = 4241.373047\n",
      "Epoch   33 loss = 4229.652344\n",
      "Epoch   34 loss = 3992.092529\n",
      "Epoch   35 loss = 3878.039795\n",
      "Epoch   36 loss = 3544.111084\n",
      "Epoch   37 loss = 3364.996338\n",
      "Epoch   38 loss = 3276.491699\n",
      "Epoch   39 loss = 3094.614746\n",
      "Epoch   40 loss = 3167.437256\n",
      "Epoch   41 loss = 3208.940674\n",
      "Epoch   42 loss = 3117.755127\n",
      "Epoch   43 loss = 2976.506348\n",
      "Epoch   44 loss = 2931.495605\n",
      "Epoch   45 loss = 2946.652344\n",
      "Epoch   46 loss = 2935.733398\n",
      "Epoch   47 loss = 2887.812256\n",
      "Epoch   48 loss = 2845.147705\n",
      "Epoch   49 loss = 2858.245361\n",
      "Epoch   50 loss = 2857.238770\n",
      "Epoch   51 loss = 2810.153809\n",
      "Epoch   52 loss = 2787.600342\n",
      "Epoch   53 loss = 2794.636963\n",
      "Epoch   54 loss = 2784.155518\n",
      "Epoch   55 loss = 2751.960693\n",
      "Epoch   56 loss = 2727.345459\n",
      "Epoch   57 loss = 2726.219971\n",
      "Epoch   58 loss = 2710.251465\n",
      "Epoch   59 loss = 2684.633545\n",
      "Epoch   60 loss = 2679.637451\n",
      "Epoch   61 loss = 2679.835693\n",
      "Epoch   62 loss = 2661.593506\n",
      "Epoch   63 loss = 2645.139404\n",
      "Epoch   64 loss = 2644.013916\n",
      "Epoch   65 loss = 2635.454834\n",
      "Epoch   66 loss = 2615.677490\n",
      "Epoch   67 loss = 2605.732178\n",
      "Epoch   68 loss = 2603.329590\n",
      "Epoch   69 loss = 2584.907471\n",
      "Epoch   70 loss = 2570.678467\n",
      "Epoch   71 loss = 2566.278076\n",
      "Epoch   72 loss = 2551.480225\n",
      "Epoch   73 loss = 2538.239746\n",
      "Epoch   74 loss = 2532.473633\n",
      "Epoch   75 loss = 2520.013428\n",
      "Epoch   76 loss = 2508.472412\n",
      "Epoch   77 loss = 2503.251465\n",
      "Epoch   78 loss = 2490.227539\n",
      "Epoch   79 loss = 2482.356201\n",
      "Epoch   80 loss = 2474.241455\n",
      "Epoch   81 loss = 2462.269043\n",
      "Epoch   82 loss = 2455.016602\n",
      "Epoch   83 loss = 2445.635010\n",
      "Epoch   84 loss = 2435.379150\n",
      "Epoch   85 loss = 2428.037842\n",
      "Epoch   86 loss = 2417.848389\n",
      "Epoch   87 loss = 2409.664062\n",
      "Epoch   88 loss = 2401.806152\n",
      "Epoch   89 loss = 2392.489258\n",
      "Epoch   90 loss = 2385.612793\n",
      "Epoch   91 loss = 2376.385986\n",
      "Epoch   92 loss = 2369.028076\n",
      "Epoch   93 loss = 2360.839600\n",
      "Epoch   94 loss = 2352.801025\n",
      "Epoch   95 loss = 2345.298096\n",
      "Epoch   96 loss = 2336.596436\n",
      "Epoch   97 loss = 2329.430908\n",
      "Epoch   98 loss = 2320.905273\n",
      "Epoch   99 loss = 2313.381348\n",
      "Epoch  100 loss = 2305.297363\n",
      "Epoch  101 loss = 2297.096436\n",
      "Epoch  102 loss = 2289.212158\n",
      "Epoch  103 loss = 2280.563477\n",
      "Epoch  104 loss = 2272.360107\n",
      "Epoch  105 loss = 2263.393311\n",
      "Epoch  106 loss = 2254.894287\n",
      "Epoch  107 loss = 2245.777100\n",
      "Epoch  108 loss = 2236.968018\n",
      "Epoch  109 loss = 2227.956787\n",
      "Epoch  110 loss = 2219.223389\n",
      "Epoch  111 loss = 2210.800293\n",
      "Epoch  112 loss = 2202.511963\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, hidden_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        return self.net(y)\n",
    "\n",
    "t = torch.from_numpy(time).float()\n",
    "y0 = torch.from_numpy(irfft(eta_hat[0, 0, :])).float().unsqueeze(0)\n",
    "y_true = torch.from_numpy(irfft(eta_hat[0, :, :])).float().unsqueeze(1)\n",
    "\n",
    "func     = ODEFunc(hidden_dim=1024)\n",
    "optimizer = torch.optim.Adam(func.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 4) Training loop\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = odeint(func, y0, t)\n",
    "\n",
    "    loss = criterion(y_pred, y_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch:4d} loss = {loss.item():.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
